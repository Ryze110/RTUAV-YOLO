# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""Convolution modules."""

import math
from typing import List

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

__all__ = (
    "Conv",
    "Conv2",
    "LightConv",
    "DWConv",
    "DWConvTranspose2d",
    "ConvTranspose",
    "Focus",
    "GhostConv",
    "ChannelAttention",
    "SpatialAttention",
    "CBAM",
    "Concat",
    "RepConv",
    "Index",
    'FCM', 'PDSConv',  'FCM_3',
    'FCM_2', 'FCM_1', 'Down', 'Conv2d_BN', 'PurePyTorchSKA', 'LKP', 'PurePyTorchLSConv', 'FLSEM_PurePyTorc',
    'SimplifiedSKA', 'SimplifiedLSConv', 'FLSEM_Simplified',
    'DynamicWeightGenerator', 'MultiScaleFeatureExtractor', 'AdaptiveFeatureModulator', 'MSFAM', 'MSFAM_Lite_1',
    'MSFAM_Lite_2',
    'MSFAM_Ultra_Lite_Adaptive',
)


def autopad(k, p=None, d=1):  # kernel, padding, dilation
    """Pad to 'same' shape outputs."""
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p


class Conv(nn.Module):
    """
    Standard convolution module with batch normalization and activation.

    Attributes:
        conv (nn.Conv2d): Convolutional layer.
        bn (nn.BatchNorm2d): Batch normalization layer.
        act (nn.Module): Activation function layer.
        default_act (nn.Module): Default activation function (SiLU).
    """

    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        """
        Initialize Conv layer with given parameters.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            k (int): Kernel size.
            s (int): Stride.
            p (int, optional): Padding.
            g (int): Groups.
            d (int): Dilation.
            act (bool | nn.Module): Activation function.
        """
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

    def forward(self, x):
        """
        Apply convolution, batch normalization and activation to input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        """
        Apply convolution and activation without batch normalization.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        return self.act(self.conv(x))


class Conv2(Conv):
    """
    Simplified RepConv module with Conv fusing.

    Attributes:
        conv (nn.Conv2d): Main 3x3 convolutional layer.
        cv2 (nn.Conv2d): Additional 1x1 convolutional layer.
        bn (nn.BatchNorm2d): Batch normalization layer.
        act (nn.Module): Activation function layer.
    """

    def __init__(self, c1, c2, k=3, s=1, p=None, g=1, d=1, act=True):
        """
        Initialize Conv2 layer with given parameters.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            k (int): Kernel size.
            s (int): Stride.
            p (int, optional): Padding.
            g (int): Groups.
            d (int): Dilation.
            act (bool | nn.Module): Activation function.
        """
        super().__init__(c1, c2, k, s, p, g=g, d=d, act=act)
        self.cv2 = nn.Conv2d(c1, c2, 1, s, autopad(1, p, d), groups=g, dilation=d, bias=False)  # add 1x1 conv

    def forward(self, x):
        """
        Apply convolution, batch normalization and activation to input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        return self.act(self.bn(self.conv(x) + self.cv2(x)))

    def forward_fuse(self, x):
        """
        Apply fused convolution, batch normalization and activation to input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        return self.act(self.bn(self.conv(x)))

    def fuse_convs(self):
        """Fuse parallel convolutions."""
        w = torch.zeros_like(self.conv.weight.data)
        i = [x // 2 for x in w.shape[2:]]
        w[:, :, i[0] : i[0] + 1, i[1] : i[1] + 1] = self.cv2.weight.data.clone()
        self.conv.weight.data += w
        self.__delattr__("cv2")
        self.forward = self.forward_fuse


class LightConv(nn.Module):
    """
    Light convolution module with 1x1 and depthwise convolutions.

    This implementation is based on the PaddleDetection HGNetV2 backbone.

    Attributes:
        conv1 (Conv): 1x1 convolution layer.
        conv2 (DWConv): Depthwise convolution layer.
    """

    def __init__(self, c1, c2, k=1, act=nn.ReLU()):
        """
        Initialize LightConv layer with given parameters.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            k (int): Kernel size for depthwise convolution.
            act (nn.Module): Activation function.
        """
        super().__init__()
        self.conv1 = Conv(c1, c2, 1, act=False)
        self.conv2 = DWConv(c2, c2, k, act=act)

    def forward(self, x):
        """
        Apply 2 convolutions to input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        return self.conv2(self.conv1(x))


class DWConv(Conv):
    """Depth-wise convolution module."""

    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):
        """
        Initialize depth-wise convolution with given parameters.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            k (int): Kernel size.
            s (int): Stride.
            d (int): Dilation.
            act (bool | nn.Module): Activation function.
        """
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)


class DWConvTranspose2d(nn.ConvTranspose2d):
    """Depth-wise transpose convolution module."""

    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):
        """
        Initialize depth-wise transpose convolution with given parameters.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            k (int): Kernel size.
            s (int): Stride.
            p1 (int): Padding.
            p2 (int): Output padding.
        """
        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))


class ConvTranspose(nn.Module):
    """
    Convolution transpose module with optional batch normalization and activation.

    Attributes:
        conv_transpose (nn.ConvTranspose2d): Transposed convolution layer.
        bn (nn.BatchNorm2d | nn.Identity): Batch normalization layer.
        act (nn.Module): Activation function layer.
        default_act (nn.Module): Default activation function (SiLU).
    """

    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=2, s=2, p=0, bn=True, act=True):
        """
        Initialize ConvTranspose layer with given parameters.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            k (int): Kernel size.
            s (int): Stride.
            p (int): Padding.
            bn (bool): Use batch normalization.
            act (bool | nn.Module): Activation function.
        """
        super().__init__()
        self.conv_transpose = nn.ConvTranspose2d(c1, c2, k, s, p, bias=not bn)
        self.bn = nn.BatchNorm2d(c2) if bn else nn.Identity()
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

    def forward(self, x):
        """
        Apply transposed convolution, batch normalization and activation to input.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        return self.act(self.bn(self.conv_transpose(x)))

    def forward_fuse(self, x):
        """
        Apply activation and convolution transpose operation to input.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        return self.act(self.conv_transpose(x))


class Focus(nn.Module):
    """
    Focus module for concentrating feature information.

    Slices input tensor into 4 parts and concatenates them in the channel dimension.

    Attributes:
        conv (Conv): Convolution layer.
    """

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):
        """
        Initialize Focus module with given parameters.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            k (int): Kernel size.
            s (int): Stride.
            p (int, optional): Padding.
            g (int): Groups.
            act (bool | nn.Module): Activation function.
        """
        super().__init__()
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act=act)
        # self.contract = Contract(gain=2)

    def forward(self, x):
        """
        Apply Focus operation and convolution to input tensor.

        Input shape is (B, C, W, H) and output shape is (B, 4C, W/2, H/2).

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        return self.conv(torch.cat((x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]), 1))
        # return self.conv(self.contract(x))


class GhostConv(nn.Module):
    """
    Ghost Convolution module.

    Generates more features with fewer parameters by using cheap operations.

    Attributes:
        cv1 (Conv): Primary convolution.
        cv2 (Conv): Cheap operation convolution.

    References:
        https://github.com/huawei-noah/Efficient-AI-Backbones
    """

    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):
        """
        Initialize Ghost Convolution module with given parameters.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            k (int): Kernel size.
            s (int): Stride.
            g (int): Groups.
            act (bool | nn.Module): Activation function.
        """
        super().__init__()
        c_ = c2 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, k, s, None, g, act=act)
        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act=act)

    def forward(self, x):
        """
        Apply Ghost Convolution to input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor with concatenated features.
        """
        y = self.cv1(x)
        return torch.cat((y, self.cv2(y)), 1)


class RepConv(nn.Module):
    """
    RepConv module with training and deploy modes.

    This module is used in RT-DETR and can fuse convolutions during inference for efficiency.

    Attributes:
        conv1 (Conv): 3x3 convolution.
        conv2 (Conv): 1x1 convolution.
        bn (nn.BatchNorm2d, optional): Batch normalization for identity branch.
        act (nn.Module): Activation function.
        default_act (nn.Module): Default activation function (SiLU).

    References:
        https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py
    """

    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=3, s=1, p=1, g=1, d=1, act=True, bn=False, deploy=False):
        """
        Initialize RepConv module with given parameters.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            k (int): Kernel size.
            s (int): Stride.
            p (int): Padding.
            g (int): Groups.
            d (int): Dilation.
            act (bool | nn.Module): Activation function.
            bn (bool): Use batch normalization for identity branch.
            deploy (bool): Deploy mode for inference.
        """
        super().__init__()
        assert k == 3 and p == 1
        self.g = g
        self.c1 = c1
        self.c2 = c2
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

        self.bn = nn.BatchNorm2d(num_features=c1) if bn and c2 == c1 and s == 1 else None
        self.conv1 = Conv(c1, c2, k, s, p=p, g=g, act=False)
        self.conv2 = Conv(c1, c2, 1, s, p=(p - k // 2), g=g, act=False)

    def forward_fuse(self, x):
        """
        Forward pass for deploy mode.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        return self.act(self.conv(x))

    def forward(self, x):
        """
        Forward pass for training mode.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Output tensor.
        """
        id_out = 0 if self.bn is None else self.bn(x)
        return self.act(self.conv1(x) + self.conv2(x) + id_out)

    def get_equivalent_kernel_bias(self):
        """
        Calculate equivalent kernel and bias by fusing convolutions.

        Returns:
            (torch.Tensor): Equivalent kernel
            (torch.Tensor): Equivalent bias
        """
        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1)
        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2)
        kernelid, biasid = self._fuse_bn_tensor(self.bn)
        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid

    @staticmethod
    def _pad_1x1_to_3x3_tensor(kernel1x1):
        """
        Pad a 1x1 kernel to 3x3 size.

        Args:
            kernel1x1 (torch.Tensor): 1x1 convolution kernel.

        Returns:
            (torch.Tensor): Padded 3x3 kernel.
        """
        if kernel1x1 is None:
            return 0
        else:
            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])

    def _fuse_bn_tensor(self, branch):
        """
        Fuse batch normalization with convolution weights.

        Args:
            branch (Conv | nn.BatchNorm2d | None): Branch to fuse.

        Returns:
            kernel (torch.Tensor): Fused kernel.
            bias (torch.Tensor): Fused bias.
        """
        if branch is None:
            return 0, 0
        if isinstance(branch, Conv):
            kernel = branch.conv.weight
            running_mean = branch.bn.running_mean
            running_var = branch.bn.running_var
            gamma = branch.bn.weight
            beta = branch.bn.bias
            eps = branch.bn.eps
        elif isinstance(branch, nn.BatchNorm2d):
            if not hasattr(self, "id_tensor"):
                input_dim = self.c1 // self.g
                kernel_value = np.zeros((self.c1, input_dim, 3, 3), dtype=np.float32)
                for i in range(self.c1):
                    kernel_value[i, i % input_dim, 1, 1] = 1
                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)
            kernel = self.id_tensor
            running_mean = branch.running_mean
            running_var = branch.running_var
            gamma = branch.weight
            beta = branch.bias
            eps = branch.eps
        std = (running_var + eps).sqrt()
        t = (gamma / std).reshape(-1, 1, 1, 1)
        return kernel * t, beta - running_mean * gamma / std

    def fuse_convs(self):
        """Fuse convolutions for inference by creating a single equivalent convolution."""
        if hasattr(self, "conv"):
            return
        kernel, bias = self.get_equivalent_kernel_bias()
        self.conv = nn.Conv2d(
            in_channels=self.conv1.conv.in_channels,
            out_channels=self.conv1.conv.out_channels,
            kernel_size=self.conv1.conv.kernel_size,
            stride=self.conv1.conv.stride,
            padding=self.conv1.conv.padding,
            dilation=self.conv1.conv.dilation,
            groups=self.conv1.conv.groups,
            bias=True,
        ).requires_grad_(False)
        self.conv.weight.data = kernel
        self.conv.bias.data = bias
        for para in self.parameters():
            para.detach_()
        self.__delattr__("conv1")
        self.__delattr__("conv2")
        if hasattr(self, "nm"):
            self.__delattr__("nm")
        if hasattr(self, "bn"):
            self.__delattr__("bn")
        if hasattr(self, "id_tensor"):
            self.__delattr__("id_tensor")


class ChannelAttention(nn.Module):
    """
    Channel-attention module for feature recalibration.

    Applies attention weights to channels based on global average pooling.

    Attributes:
        pool (nn.AdaptiveAvgPool2d): Global average pooling.
        fc (nn.Conv2d): Fully connected layer implemented as 1x1 convolution.
        act (nn.Sigmoid): Sigmoid activation for attention weights.

    References:
        https://github.com/open-mmlab/mmdetection/tree/v3.0.0rc1/configs/rtmdet
    """

    def __init__(self, channels: int) -> None:
        """
        Initialize Channel-attention module.

        Args:
            channels (int): Number of input channels.
        """
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)
        self.act = nn.Sigmoid()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Apply channel attention to input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Channel-attended output tensor.
        """
        return x * self.act(self.fc(self.pool(x)))


class SpatialAttention(nn.Module):
    """
    Spatial-attention module for feature recalibration.

    Applies attention weights to spatial dimensions based on channel statistics.

    Attributes:
        cv1 (nn.Conv2d): Convolution layer for spatial attention.
        act (nn.Sigmoid): Sigmoid activation for attention weights.
    """

    def __init__(self, kernel_size=7):
        """
        Initialize Spatial-attention module.

        Args:
            kernel_size (int): Size of the convolutional kernel (3 or 7).
        """
        super().__init__()
        assert kernel_size in {3, 7}, "kernel size must be 3 or 7"
        padding = 3 if kernel_size == 7 else 1
        self.cv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.act = nn.Sigmoid()

    def forward(self, x):
        """
        Apply spatial attention to input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Spatial-attended output tensor.
        """
        return x * self.act(self.cv1(torch.cat([torch.mean(x, 1, keepdim=True), torch.max(x, 1, keepdim=True)[0]], 1)))


class CBAM(nn.Module):
    """
    Convolutional Block Attention Module.

    Combines channel and spatial attention mechanisms for comprehensive feature refinement.

    Attributes:
        channel_attention (ChannelAttention): Channel attention module.
        spatial_attention (SpatialAttention): Spatial attention module.
    """

    def __init__(self, c1, kernel_size=7):
        """
        Initialize CBAM with given parameters.

        Args:
            c1 (int): Number of input channels.
            kernel_size (int): Size of the convolutional kernel for spatial attention.
        """
        super().__init__()
        self.channel_attention = ChannelAttention(c1)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        """
        Apply channel and spatial attention sequentially to input tensor.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            (torch.Tensor): Attended output tensor.
        """
        return self.spatial_attention(self.channel_attention(x))


class Concat(nn.Module):
    """
    Concatenate a list of tensors along specified dimension.

    Attributes:
        d (int): Dimension along which to concatenate tensors.
    """

    def __init__(self, dimension=1):
        """
        Initialize Concat module.

        Args:
            dimension (int): Dimension along which to concatenate tensors.
        """
        super().__init__()
        self.d = dimension

    def forward(self, x: List[torch.Tensor]):
        """
        Concatenate input tensors along specified dimension.

        Args:
            x (List[torch.Tensor]): List of input tensors.

        Returns:
            (torch.Tensor): Concatenated tensor.
        """
        return torch.cat(x, self.d)


class Index(nn.Module):
    """
    Returns a particular index of the input.

    Attributes:
        index (int): Index to select from input.
    """

    def __init__(self, index=0):
        """
        Initialize Index module.

        Args:
            index (int): Index to select from input.
        """
        super().__init__()
        self.index = index

    def forward(self, x: List[torch.Tensor]):
        """
        Select and return a particular index from input.

        Args:
            x (List[torch.Tensor]): List of input tensors.

        Returns:
            (torch.Tensor): Selected tensor.
        """
        return x[self.index]


class DWConv_FCM(nn.Module):
    """Depthwise Conv + Conv for FCM"""
    def __init__(self, in_channels):
        super().__init__()
        self.dconv = nn.Conv2d(
            in_channels, in_channels, 3,
            1, 1, groups=in_channels
        )

    def forward(self, x):
        x = self.dconv(x)
        return x


class Channel(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dwconv = nn.Conv2d(
            dim, dim, 3,
            1, 1, groups=dim
        )
        self.Apt = nn.AdaptiveAvgPool2d(1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x2 = self.dwconv(x)
        x5 = self.Apt(x2)
        x6 = self.sigmoid(x5)
        return x6


class Spatial(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.conv1 = nn.Conv2d(dim, 1, 1, 1)
        self.bn = nn.BatchNorm2d(1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x1 = self.conv1(x)
        x5 = self.bn(x1)
        x6 = self.sigmoid(x5)
        return x6


class FCM_3(nn.Module):
    def __init__(self, c1, c2, *args):
        super().__init__()
        # c1 æ˜¯è¾“å…¥é€šé“æ•°ï¼Œc2 æ˜¯è¾“å‡ºé€šé“æ•°
        dim = c1  # ä½¿ç”¨è¾“å…¥é€šé“æ•°ä½œä¸º dim
        
        self.one = dim - dim // 4
        self.two = dim // 4
        self.conv1 = Conv(dim - dim // 4, dim - dim // 4, 3, 1, 1)
        self.conv12 = Conv(dim - dim // 4, dim - dim // 4, 3, 1, 1)
        self.conv123 = Conv(dim - dim // 4, dim, 1, 1)
        self.conv2 = Conv(dim // 4, dim, 1, 1)
        self.spatial = Spatial(dim)
        self.channel = Channel(dim)

    def forward(self, x):
        x1, x2 = torch.split(x, [self.one, self.two], dim=1)
        x3 = self.conv1(x1)
        x3 = self.conv12(x3)
        x3 = self.conv123(x3)
        x4 = self.conv2(x2)
        x33 = self.spatial(x4) * x3
        x44 = self.channel(x3) * x4
        x5 = x33 + x44
        return x5


class FCM_2(nn.Module):
    def __init__(self, c1, c2, *args):
        super().__init__()
        # c1 æ˜¯è¾“å…¥é€šé“æ•°ï¼Œc2 æ˜¯è¾“å‡ºé€šé“æ•°
        dim = c1  # ä½¿ç”¨è¾“å…¥é€šé“æ•°ä½œä¸º dim
        
        self.one = dim - dim // 4
        self.two = dim // 4
        self.conv1 = Conv(dim - dim // 4, dim - dim // 4, 3, 1, 1)
        self.conv12 = Conv(dim - dim // 4, dim - dim // 4, 3, 1, 1)
        self.conv123 = Conv(dim - dim // 4, dim, 1, 1)
        self.conv2 = Conv(dim // 4, dim, 1, 1)
        self.spatial = Spatial(dim)
        self.channel = Channel(dim)

    def forward(self, x):
        x1, x2 = torch.split(x, [self.one, self.two], dim=1)
        x3 = self.conv1(x1)
        x3 = self.conv12(x3)
        x3 = self.conv123(x3)
        x4 = self.conv2(x2)
        x33 = self.spatial(x4) * x3
        x44 = self.channel(x3) * x4
        x5 = x33 + x44
        return x5


class FCM_1(nn.Module):
    def __init__(self, c1, c2, *args):
        super().__init__()
        # c1 æ˜¯è¾“å…¥é€šé“æ•°ï¼Œc2 æ˜¯è¾“å‡ºé€šé“æ•°
        dim = c1  # ä½¿ç”¨è¾“å…¥é€šé“æ•°ä½œä¸º dim
        
        self.one = dim // 4
        self.two = dim - dim // 4
        self.conv1 = Conv(dim // 4, dim // 4, 3, 1, 1)
        self.conv12 = Conv(dim // 4, dim // 4, 3, 1, 1)
        self.conv123 = Conv(dim // 4, dim, 1, 1)
        self.conv2 = Conv(dim - dim // 4, dim, 1, 1)
        self.spatial = Spatial(dim)
        self.channel = Channel(dim)

    def forward(self, x):
        x1, x2 = torch.split(x, [self.one, self.two], dim=1)
        x3 = self.conv1(x1)
        x3 = self.conv12(x3)
        x3 = self.conv123(x3)
        x4 = self.conv2(x2)
        x33 = self.spatial(x4) * x3
        x44 = self.channel(x3) * x4
        x5 = x33 + x44
        return x5


class FCM(nn.Module):
    def __init__(self, c1, c2, *args):
        super().__init__()
        # c1 æ˜¯è¾“å…¥é€šé“æ•°ï¼Œc2 æ˜¯è¾“å‡ºé€šé“æ•°
        # å¦‚æœæœ‰é¢å¤–å‚æ•°ï¼Œå¯ä»¥åœ¨è¿™é‡Œå¤„ç†
        dim = c1  # ä½¿ç”¨è¾“å…¥é€šé“æ•°ä½œä¸º dim
        dim_out = c2  # ä½¿ç”¨è¾“å‡ºé€šé“æ•°ä½œä¸º dim_out
        
        self.one = dim // 4
        self.two = dim - dim // 4
        self.conv1 = Conv(dim // 4, dim // 4, 3, 1, 1)
        self.conv12 = Conv(dim // 4, dim // 4, 3, 1, 1)
        self.conv123 = Conv(dim // 4, dim, 1, 1)
        self.conv2 = Conv(dim - dim // 4, dim, 1, 1)
        self.conv3 = Conv(dim, c2, 1, 1)  # è¾“å‡ºåˆ°æŒ‡å®šé€šé“æ•°
        self.spatial = Spatial(dim)
        self.channel = Channel(dim)

    def forward(self, x):
        x1, x2 = torch.split(x, [self.one, self.two], dim=1)
        x3 = self.conv1(x1)
        x3 = self.conv12(x3)
        x3 = self.conv123(x3)
        x4 = self.conv2(x2)
        x33 = self.spatial(x4) * x3
        x44 = self.channel(x3) * x4
        x5 = x33 + x44
        x5 = self.conv3(x5)
        return x5

class PDSConv(nn.Module):
    ##PDSConv (Progressive Dilated Separable Convolution)
    def __init__(self, dim, k=1, s=1, p=None, g=1, d=1, act=True):
        super().__init__()
        self.conv1 = nn.Conv2d(
            dim, dim, 3,
            1, 1, groups=dim, dilation=1
        )
        self.conv2 = Conv(dim, dim, k=1, s=1, )
        self.conv3 = nn.Conv2d(
            dim, dim, 3,
            1, 2, groups=dim, dilation=2
        )
        self.conv4 = Conv(dim, dim, 1, 1)
        self.conv5 = nn.Conv2d(
            dim, dim, 3,
            1, 3, groups=dim ,dilation=3
        )

    def forward(self, x):
        x1 = self.conv1(x)
        x2 = self.conv2(x1)
        x3 = self.conv3(x2)
        x4 = self.conv4(x3)
        x5 = self.conv5(x4)
        x6 = x5 + x
        return x6
    
class Down(nn.Module):
    def __init__(self, dim, dim_out):
        super().__init__()
        self.conv2 = Conv(dim, dim, 3, 2, 1, g=dim // 2, act=False)
        self.conv4 = Conv(dim, dim_out, 1, 1)

    def forward(self, x):
        x2 = self.conv2(x)
        x2 = self.conv4(x2)
        return x2

class Conv2d_BN(torch.nn.Sequential):
    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,
                 groups=1, bn_weight_init=1):
        super().__init__()
        self.add_module('c', torch.nn.Conv2d(
            a, b, ks, stride, pad, dilation, groups, bias=False))
        self.add_module('bn', torch.nn.BatchNorm2d(b))
        torch.nn.init.constant_(self.bn.weight, bn_weight_init)
        torch.nn.init.constant_(self.bn.bias, 0)


class PurePyTorchSKA(nn.Module):
    """
    çº¯PyTorchå®ç°çš„ç©ºé—´æ ¸æ³¨æ„åŠ›ï¼ˆSKAï¼‰
    æ›¿ä»£åŸå§‹çš„Tritonå®ç°ï¼Œä½¿ç”¨æ ‡å‡†PyTorchæ“ä½œ
    """
    def __init__(self, kernel_size=3):
        super().__init__()
        self.kernel_size = kernel_size
        self.padding = (kernel_size - 1) // 2
        
    def forward(self, x, weights):
        """
        ä½¿ç”¨åŠ¨æ€å·ç§¯æƒé‡å¯¹è¾“å…¥è¿›è¡Œå·ç§¯
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [B, C, H, W]
            weights: åŠ¨æ€æƒé‡ [B, C//groups, K*K, H, W]
        
        Returns:
            è¾“å‡ºç‰¹å¾ [B, C, H, W]
        """
        B, C, H, W = x.shape
        _, wc, kk, _, _ = weights.shape
        
        # è®¡ç®—åˆ†ç»„æ•°
        groups = C // wc
        
        # å°†è¾“å…¥æŒ‰ç»„é‡å¡‘
        x_grouped = x.view(B, groups, wc, H, W)
        
        # ä½¿ç”¨unfoldæå–æ»‘åŠ¨çª—å£
        x_unfolded = F.unfold(x, kernel_size=self.kernel_size, padding=self.padding)
        # x_unfolded: [B, C*K*K, H*W]
        
        x_unfolded = x_unfolded.view(B, C, kk, H, W)
        x_unfolded = x_unfolded.view(B, groups, wc, kk, H, W)
        
        # åº”ç”¨åŠ¨æ€æƒé‡
        # weights: [B, wc, kk, H, W] -> [B, 1, wc, kk, H, W]
        weights_expanded = weights.unsqueeze(1)  # [B, 1, wc, kk, H, W]
        
        # é€å…ƒç´ ç›¸ä¹˜å¹¶æ±‚å’Œ
        output = (x_unfolded * weights_expanded).sum(dim=3)  # [B, groups, wc, H, W]
        
        # é‡å¡‘å›åŸå§‹å½¢çŠ¶
        output = output.view(B, C, H, W)
        
        return output


class LKP(nn.Module):
    """å¤§æ ¸å‚æ•°ç”Ÿæˆå™¨ - çº¯PyTorchç‰ˆæœ¬"""
    def __init__(self, dim, lks, sks, groups):
        super().__init__()
        self.cv1 = Conv2d_BN(dim, dim // 2)
        self.act = nn.ReLU()
        self.cv2 = Conv2d_BN(dim // 2, dim // 2, ks=lks, pad=(lks - 1) // 2, groups=dim // 2)
        self.cv3 = Conv2d_BN(dim // 2, dim // 2)
        self.cv4 = nn.Conv2d(dim // 2, sks ** 2 * dim // groups, kernel_size=1)
        self.norm = nn.GroupNorm(num_groups=dim // groups, num_channels=sks ** 2 * dim // groups)
        
        self.sks = sks
        self.groups = groups
        self.dim = dim
        
    def forward(self, x):
        x = self.act(self.cv3(self.cv2(self.act(self.cv1(x)))))
        w = self.norm(self.cv4(x))
        b, _, h, width = w.size()
        w = w.view(b, self.dim // self.groups, self.sks ** 2, h, width)
        return w


class PurePyTorchLSConv(nn.Module):
    """çº¯PyTorchå®ç°çš„LSConvæ¨¡å—"""
    def __init__(self, dim, kernel_size=3):
        super(PurePyTorchLSConv, self).__init__()
        self.lkp = LKP(dim, lks=7, sks=kernel_size, groups=max(1, dim // 8))
        self.ska = PurePyTorchSKA(kernel_size=kernel_size)
        self.bn = nn.BatchNorm2d(dim)

    def forward(self, x):
        weights = self.lkp(x)
        output = self.ska(x, weights)
        return self.bn(output) + x

# ============ çº¯PyTorchç‰ˆæœ¬çš„FLSEM ============
class FLSEM_PurePyTorch(nn.Module):
    """
    FLSEMçš„çº¯PyTorchå®ç°ç‰ˆæœ¬
    å»é™¤äº†Tritonä¾èµ–ï¼Œä½¿ç”¨æ ‡å‡†PyTorchæ“ä½œå®ç°ç›¸åŒåŠŸèƒ½
    
    è®¾è®¡ç†å¿µ:
    1. ä¿æŒåŸæœ‰çš„åŒè·¯å¾„æ¶æ„å’Œæ³¨æ„åŠ›æœºåˆ¶
    2. ä½¿ç”¨çº¯PyTorchå®ç°ç©ºé—´æ ¸æ³¨æ„åŠ›åŠŸèƒ½
    3. ç¡®ä¿ä¸åŸç‰ˆæœ¬åŠŸèƒ½ç­‰ä»·ä½†æ— å¤–éƒ¨ä¾èµ–
    4. ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œé€‚åˆCPUå’ŒGPUéƒ¨ç½²
    
    æŠ€æœ¯ä¼˜åŠ¿:
    - æ— Tritonä¾èµ–ï¼šå¯åœ¨ä»»ä½•æ”¯æŒPyTorchçš„ç¯å¢ƒè¿è¡Œ
    - è·¨å¹³å°å…¼å®¹ï¼šæ”¯æŒCPUã€GPUã€ç§»åŠ¨ç«¯éƒ¨ç½²
    - æ˜“äºè°ƒè¯•ï¼šä½¿ç”¨æ ‡å‡†PyTorchæ“ä½œï¼Œä¾¿äºé—®é¢˜å®šä½
    - éƒ¨ç½²å‹å¥½ï¼šå‡å°‘ä¾èµ–å¤æ‚åº¦ï¼Œæå‡éƒ¨ç½²æˆåŠŸç‡
    """
    
    def __init__(self, dim, dim_out):
        super().__init__()
        # é€šé“åˆ†å‰²æ¯”ä¾‹
        self.split_ratio = dim // 4
        self.remaining = dim - self.split_ratio
        
        # çº¯PyTorch LSConvåˆ†æ”¯
        self.lsconv = PurePyTorchLSConv(self.split_ratio, kernel_size=3)
        
        # FCMåˆ†æ”¯ - è´Ÿè´£æ³¨æ„åŠ›è°ƒåˆ¶å’Œç‰¹å¾èåˆ
        self.fcm_conv1 = Conv(self.remaining, self.remaining, 3, 1, 1)
        self.fcm_conv2 = Conv(self.remaining, dim, 1, 1)
        
        # æ³¨æ„åŠ›æ¨¡å—
        self.spatial_att = Spatial(dim)
        self.channel_att = Channel(dim)
        
        # ç‰¹å¾èåˆå’Œè¾“å‡º
        self.fusion_conv = Conv(dim, dim, 1, 1)
        self.output_conv = Conv(dim, dim, 3, 1, 1)
        
        # å½’ä¸€åŒ–å±‚
        self.bn1 = nn.BatchNorm2d(self.split_ratio)
        self.bn2 = nn.BatchNorm2d(dim)
        
    def forward(self, x):
        # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        identity = x
        
        # é€šé“åˆ†å‰²ï¼šå°éƒ¨åˆ†ç”¨äºLSConvï¼Œå¤§éƒ¨åˆ†ç”¨äºFCMå¤„ç†
        if self.split_ratio > 0:
            x_lsconv, x_fcm = torch.split(x, [self.split_ratio, self.remaining], dim=1)
            
            # LSConvåˆ†æ”¯ï¼šå¤§æ„Ÿå—é‡ç‰¹å¾æå–ï¼ˆçº¯PyTorchå®ç°ï¼‰
            x_lsconv_enhanced = self.lsconv(x_lsconv)
        else:
            x_lsconv_enhanced = None
            x_fcm = x
        
        # FCMåˆ†æ”¯ï¼šç‰¹å¾å¤„ç†å’Œç»´åº¦æ‰©å±•
        x_fcm_processed = self.fcm_conv1(x_fcm)
        x_fcm_expanded = self.fcm_conv2(x_fcm_processed)
        
        # ç‰¹å¾é‡ç»„ï¼šå°†LSConvå¢å¼ºçš„ç‰¹å¾ä¸FCMå¤„ç†çš„ç‰¹å¾èåˆ
        if x_lsconv_enhanced is not None and x_lsconv_enhanced.size(1) > 0:
            # å°†LSConvç‰¹å¾æ‰©å±•åˆ°ä¸FCMç‰¹å¾ç›¸åŒçš„ç»´åº¦
            repeat_times = self.remaining // self.split_ratio
            if repeat_times > 0:
                x_lsconv_expanded = torch.cat([
                    x_lsconv_enhanced, 
                    x_lsconv_enhanced.repeat(1, repeat_times, 1, 1)
                ], dim=1)
            else:
                x_lsconv_expanded = x_lsconv_enhanced
                
            # è°ƒæ•´åˆ°ç›®æ ‡ç»´åº¦
            target_dim = x_fcm_expanded.size(1)
            if x_lsconv_expanded.size(1) > target_dim:
                x_lsconv_expanded = x_lsconv_expanded[:, :target_dim, :, :]
            elif x_lsconv_expanded.size(1) < target_dim:
                padding = target_dim - x_lsconv_expanded.size(1)
                x_lsconv_expanded = torch.cat([
                    x_lsconv_expanded,
                    x_lsconv_enhanced[:, :padding, :, :]
                ], dim=1)
        else:
            x_lsconv_expanded = torch.zeros_like(x_fcm_expanded)
        
        # ç‰¹å¾èåˆ
        fused_features = self.fusion_conv(x_fcm_expanded + x_lsconv_expanded)
        
        # åŒé‡æ³¨æ„åŠ›è°ƒåˆ¶
        spatial_weight = self.spatial_att(fused_features)
        channel_weight = self.channel_att(fused_features)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attended_features = fused_features * spatial_weight * channel_weight
        
        # æœ€ç»ˆç‰¹å¾å¤„ç†
        output = self.output_conv(attended_features)
        output = self.bn2(output)
        
        # æ®‹å·®è¿æ¥
        return output + identity


class SimplifiedSKA(nn.Module):
    """
    ç®€åŒ–ç‰ˆç©ºé—´æ ¸æ³¨æ„åŠ›
    ä½¿ç”¨æ›´ç›´æ¥çš„æ–¹æ³•å®ç°åŠ¨æ€å·ç§¯
    """
    def __init__(self, kernel_size=3):
        super().__init__()
        self.kernel_size = kernel_size
        self.padding = (kernel_size - 1) // 2
        
    def forward(self, x, weights):
        B, C, H, W = x.shape
        _, wc, kk, _, _ = weights.shape
        
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨åˆ†ç»„å·ç§¯è¿‘ä¼¼åŠ¨æ€å·ç§¯
        groups = C // wc
        
        # å°†æƒé‡é‡å¡‘ä¸ºå·ç§¯æ ¸æ ¼å¼
        # weights: [B, wc, kk, H, W] -> [B*wc, 1, k, k, H, W]
        k = int(kk ** 0.5)
        weights_reshaped = weights.view(B, wc, k, k, H, W)
        
        # ä½¿ç”¨å¹³å‡æƒé‡ä½œä¸ºé™æ€å·ç§¯æ ¸ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        avg_weights = weights_reshaped.mean(dim=(4, 5))  # [B, wc, k, k]
        
        # åº”ç”¨åˆ†ç»„å·ç§¯
        output = []
        for i in range(groups):
            start_ch = i * wc
            end_ch = start_ch + wc
            x_group = x[:, start_ch:end_ch, :, :]
            
            # ä½¿ç”¨å¹³å‡æƒé‡è¿›è¡Œå·ç§¯ï¼ˆç®€åŒ–å®ç°ï¼‰
            group_output = F.conv2d(
                x_group.view(-1, wc, H, W),
                avg_weights.view(-1, 1, k, k),
                padding=self.padding,
                groups=B * wc
            )
            group_output = group_output.view(B, wc, H, W)
            output.append(group_output)
        
        return torch.cat(output, dim=1)


class SimplifiedLSConv(nn.Module):
    """ç®€åŒ–ç‰ˆLSConvï¼Œæ›´é€‚åˆCPUéƒ¨ç½²"""
    def __init__(self, dim):
        super().__init__()
        # ä½¿ç”¨æ ‡å‡†å·ç§¯æ›¿ä»£å¤æ‚çš„åŠ¨æ€å·ç§¯
        self.large_conv = nn.Conv2d(dim, dim, 7, 1, 3, groups=dim)
        self.point_conv = nn.Conv2d(dim, dim, 1, 1, 0)
        self.bn = nn.BatchNorm2d(dim)
        self.act = nn.SiLU()
        
    def forward(self, x):
        # å¤§æ ¸å·ç§¯ + ç‚¹å·ç§¯çš„ç»„åˆ
        out = self.large_conv(x)
        out = self.point_conv(out)
        out = self.bn(out)
        out = self.act(out)
        return out + x


class FLSEM_Simplified(nn.Module):
    """
    FLSEMç®€åŒ–ç‰ˆæœ¬
    ä½¿ç”¨æ›´ç®€å•çš„æ“ä½œï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒ
    """
    def __init__(self, dim, dimout):
        super().__init__()
        self.split_ratio = dim // 2
        self.remaining = dim - self.split_ratio
        
        # ç®€åŒ–çš„å¤§æ ¸åˆ†æ”¯
        if self.split_ratio > 0:
            self.large_kernel_branch = SimplifiedLSConv(self.split_ratio)
        
        # FCMåˆ†æ”¯
        self.fcm_conv1 = Conv(self.remaining, self.remaining, 3, 1, 1)
        self.fcm_conv2 = Conv(self.remaining, dim, 1, 1)
        
        # æ³¨æ„åŠ›æ¨¡å—
        self.spatial_att = Spatial(dim)
        self.channel_att = Channel(dim)
        
        # è¾“å‡ºå±‚
        self.output_conv = Conv(dim, dim, 1, 1)
        
    def forward(self, x):
        identity = x
        
        if self.split_ratio > 0:
            x_large, x_fcm = torch.split(x, [self.split_ratio, self.remaining], dim=1)
            x_large_enhanced = self.large_kernel_branch(x_large)
        else:
            x_large_enhanced = None
            x_fcm = x
        
        # FCMå¤„ç†
        x_fcm_processed = self.fcm_conv1(x_fcm)
        x_fcm_expanded = self.fcm_conv2(x_fcm_processed)
        
        # ç‰¹å¾èåˆ
        if x_large_enhanced is not None:
            # ç®€å•çš„ç‰¹å¾æ‹¼æ¥å’Œç»´åº¦è°ƒæ•´
            repeat_times = self.remaining // self.split_ratio
            if repeat_times > 0:
                x_large_expanded = x_large_enhanced.repeat(1, repeat_times + 1, 1, 1)
                target_dim = x_fcm_expanded.size(1)
                x_large_expanded = x_large_expanded[:, :target_dim, :, :]
            else:
                x_large_expanded = torch.zeros_like(x_fcm_expanded)
            
            fused = x_fcm_expanded + x_large_expanded
        else:
            fused = x_fcm_expanded
        
        # æ³¨æ„åŠ›è°ƒåˆ¶
        spatial_weight = self.spatial_att(fused)
        channel_weight = self.channel_att(fused)
        attended = fused * spatial_weight * channel_weight
        
        # è¾“å‡º
        output = self.output_conv(attended)
        return output + identity


class DynamicWeightGenerator(nn.Module):
    """åŠ¨æ€æƒé‡ç”Ÿæˆå™¨ - MSFAMçš„æ ¸å¿ƒç»„ä»¶"""
    def __init__(self, channels, reduction=8):
        super().__init__()
        self.channels = channels
        self.reduction = reduction
        
        # å…¨å±€ç‰¹å¾æå–
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.global_fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )
        
        # å±€éƒ¨ç‰¹å¾æå–
        self.local_conv = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, 1),
            nn.BatchNorm2d(channels // reduction),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1),
            nn.Sigmoid()
        )
        
        # åŠ¨æ€èåˆæƒé‡
        self.fusion_weight = nn.Parameter(torch.ones(2))
        
    def forward(self, x):
        B, C, H, W = x.shape
        
        # å…¨å±€æƒé‡
        global_feat = self.global_pool(x).view(B, C)
        global_weight = self.global_fc(global_feat).view(B, C, 1, 1)
        
        # å±€éƒ¨æƒé‡
        local_weight = self.local_conv(x)
        
        # åŠ¨æ€èåˆ
        fusion_weights = F.softmax(self.fusion_weight, dim=0)
        dynamic_weight = fusion_weights[0] * global_weight + fusion_weights[1] * local_weight
        
        return dynamic_weight

class MultiScaleFeatureExtractor(nn.Module):
    """å¤šå°ºåº¦ç‰¹å¾æå–å™¨"""
    def __init__(self, channels):
        super().__init__()
        self.channels = channels
        
        # ä¸åŒå°ºåº¦çš„ç‰¹å¾æå–åˆ†æ”¯
        self.scale1 = nn.Sequential(
            nn.Conv2d(channels, channels // 4, 1),
            nn.BatchNorm2d(channels // 4),
            nn.ReLU(inplace=True)
        )
        
        self.scale2 = nn.Sequential(
            nn.Conv2d(channels, channels // 4, 1),
            nn.BatchNorm2d(channels // 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // 4, channels // 4, 3, padding=1),
            nn.BatchNorm2d(channels // 4),
            nn.ReLU(inplace=True)
        )
        
        self.scale3 = nn.Sequential(
            nn.Conv2d(channels, channels // 4, 1),
            nn.BatchNorm2d(channels // 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // 4, channels // 4, 3, padding=2, dilation=2),
            nn.BatchNorm2d(channels // 4),
            nn.ReLU(inplace=True)
        )
        
        self.scale4 = nn.Sequential(
            nn.Conv2d(channels, channels // 4, 1),
            nn.BatchNorm2d(channels // 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // 4, channels // 4, 3, padding=3, dilation=3),
            nn.BatchNorm2d(channels // 4),
            nn.ReLU(inplace=True)
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(channels, channels, 1),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True)
        )
        
    def forward(self, x):
        # å¤šå°ºåº¦ç‰¹å¾æå–
        feat1 = self.scale1(x)  # 1x1 ç»†èŠ‚ç‰¹å¾
        feat2 = self.scale2(x)  # 3x3 å±€éƒ¨ç‰¹å¾
        feat3 = self.scale3(x)  # ç©ºæ´å·ç§¯ ä¸­ç­‰æ„Ÿå—é‡
        feat4 = self.scale4(x)  # ç©ºæ´å·ç§¯ å¤§æ„Ÿå—é‡
        
        # ç‰¹å¾æ‹¼æ¥å’Œèåˆ
        multi_scale_feat = torch.cat([feat1, feat2, feat3, feat4], dim=1)
        fused_feat = self.fusion(multi_scale_feat)
        
        return fused_feat

class AdaptiveFeatureModulator(nn.Module):
    """è‡ªé€‚åº”ç‰¹å¾è°ƒåˆ¶å™¨"""
    def __init__(self, channels):
        super().__init__()
        self.channels = channels
        
        # ç©ºé—´æ³¨æ„åŠ›åˆ†æ”¯
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(channels, 1, 7, padding=3),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        
        # é€šé“æ³¨æ„åŠ›åˆ†æ”¯
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels // 8, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // 8, channels, 1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾é‡æ„ç½‘ç»œ
        self.feature_reconstruct = nn.Sequential(
            nn.Conv2d(channels * 2, channels, 1),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels)
        )
        
    def forward(self, original_feat, enhanced_feat, dynamic_weight):
        # åº”ç”¨åŠ¨æ€æƒé‡
        weighted_enhanced = enhanced_feat * dynamic_weight
        
        # ç©ºé—´å’Œé€šé“æ³¨æ„åŠ›
        spatial_att = self.spatial_attention(weighted_enhanced)
        channel_att = self.channel_attention(weighted_enhanced)
        
        # æ³¨æ„åŠ›åŠ æƒ
        attended_feat = weighted_enhanced * spatial_att * channel_att
        
        # ç‰¹å¾é‡æ„
        combined_feat = torch.cat([original_feat, attended_feat], dim=1)
        reconstructed_feat = self.feature_reconstruct(combined_feat)
        
        return reconstructed_feat

class MSFAM(nn.Module):
    """å¤šå°ºåº¦ç‰¹å¾è‡ªé€‚åº”è°ƒåˆ¶æ¨¡å— (Multi-Scale Feature Adaptive Modulation)
    
    è®ºæ–‡è´¡çŒ®ç‚¹ï¼š
    1. æå‡ºåŠ¨æ€æƒé‡ç”Ÿæˆæœºåˆ¶ï¼Œæ ¹æ®è¾“å…¥ç‰¹å¾è‡ªé€‚åº”è°ƒæ•´å¤„ç†ç­–ç•¥
    2. è®¾è®¡å¤šå°ºåº¦ç‰¹å¾æå–å™¨ï¼Œæœ‰æ•ˆèåˆä¸åŒæ„Ÿå—é‡çš„ç‰¹å¾ä¿¡æ¯
    3. å¼•å…¥è‡ªé€‚åº”ç‰¹å¾è°ƒåˆ¶å™¨ï¼Œå®ç°ç‰¹å¾çš„æ™ºèƒ½é‡æ„å’Œå¢å¼º
    4. ä¸“é—¨é’ˆå¯¹å°ç›®æ ‡æ£€æµ‹è¿›è¡Œä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½
    
    æŠ€æœ¯åˆ›æ–°ï¼š
    - åŠ¨æ€æƒé‡ç”Ÿæˆï¼šæ‘†è„±å›ºå®šæƒé‡é™åˆ¶ï¼Œå®ç°è‡ªé€‚åº”ç‰¹å¾å¤„ç†
    - å¤šå°ºåº¦èåˆï¼šç»“åˆå±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¢å¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›
    - æ³¨æ„åŠ›å¼•å¯¼ï¼šé€šè¿‡ç©ºé—´å’Œé€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œçªå‡ºé‡è¦ç‰¹å¾
    - ç‰¹å¾é‡æ„ï¼šæ™ºèƒ½é‡ç»„ç‰¹å¾è¡¨ç¤ºï¼Œæå‡æ¨¡å‹åˆ¤åˆ«èƒ½åŠ›
    """
    
    def __init__(self, c1, c2, reduction=8):
        super().__init__()
        self.c1 = c1
        self.c2 = c2
        
        # è¾“å…¥ç‰¹å¾é¢„å¤„ç†
        self.input_proj = Conv(c1, c1, 1, 1) if c1 == c2 else Conv(c1, c2, 1, 1)
        
        # æ ¸å¿ƒç»„ä»¶
        self.dynamic_weight_gen = DynamicWeightGenerator(c2, reduction)
        self.multi_scale_extractor = MultiScaleFeatureExtractor(c2)
        self.adaptive_modulator = AdaptiveFeatureModulator(c2)
        
        # è¾“å‡ºå¤„ç†
        self.output_conv = Conv(c2, c2, 3, 1, 1)
        
        # æ®‹å·®è¿æ¥
        self.use_residual = (c1 == c2)
        if not self.use_residual:
            self.residual_proj = Conv(c1, c2, 1, 1)
            
    def forward(self, x):
        # è¾“å…¥é¢„å¤„ç†
        projected_x = self.input_proj(x)
        
        # åŠ¨æ€æƒé‡ç”Ÿæˆ
        dynamic_weight = self.dynamic_weight_gen(projected_x)
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        multi_scale_feat = self.multi_scale_extractor(projected_x)
        
        # è‡ªé€‚åº”ç‰¹å¾è°ƒåˆ¶
        modulated_feat = self.adaptive_modulator(projected_x, multi_scale_feat, dynamic_weight)
        
        # è¾“å‡ºå¤„ç†
        output = self.output_conv(modulated_feat)
        
        # æ®‹å·®è¿æ¥
        if self.use_residual:
            output = output + x
        elif hasattr(self, 'residual_proj'):
            output = output + self.residual_proj(x)
            
        return output

# è½»é‡çº§ç‰ˆæœ¬
class MSFAM_Lite_1(nn.Module):
    """MSFAMè½»é‡çº§ç‰ˆæœ¬ - é€‚ç”¨äºå®æ—¶æ£€æµ‹åœºæ™¯ï¼Œæµ…å±‚"""
    
    def __init__(self, c1, c2, reduction=8):
        super().__init__()
        self.c1 = c1
        self.c2 = c2
        
        # ç®€åŒ–çš„åŠ¨æ€æƒé‡ç”Ÿæˆ
        self.weight_gen = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(c1, c1 // reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(c1 // reduction, c2, 1),
            nn.Sigmoid()
        )
        
        # è®¡ç®—é€šé“æ•°ï¼ˆç¡®ä¿ä¸ºæ•´æ•°ï¼‰
        branch1_channels = int(c2 * 0.75)  # è½¬æ¢ä¸ºæ•´æ•°
        branch2_channels = c2 - branch1_channels  # ç¡®ä¿æ€»å’Œç­‰äºc2
        
        # åŒåˆ†æ”¯ç‰¹å¾æå–
        self.branch1 = nn.Sequential(
            Conv(c1, branch1_channels, 1, 1),
            Conv(branch1_channels, branch1_channels, 3, 1, 1)
        )
        
        self.branch2 = nn.Sequential(
            Conv(c1, branch2_channels, 1, 1),
            nn.Conv2d(branch2_channels, branch2_channels, 3, 1, 2, dilation=2)
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = Conv(c2, c2, 1, 1)
        
        # æ®‹å·®è¿æ¥
        self.use_residual = (c1 == c2)
        if not self.use_residual:
            self.residual_proj = Conv(c1, c2, 1, 1)
            
    def forward(self, x):
        # åŠ¨æ€æƒé‡
        weight = self.weight_gen(x)
        
        # åŒåˆ†æ”¯å¤„ç†
        feat1 = self.branch1(x)
        feat2 = self.branch2(x)
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([feat1, feat2], dim=1)
        output = self.fusion(combined) * weight
        
        # æ®‹å·®è¿æ¥
        if self.use_residual:
            output = output + x
        elif hasattr(self, 'residual_proj'):
            output = output + self.residual_proj(x)
            
        return output

class MSFAM_Lite_2(nn.Module):
    """MSFAMè½»é‡çº§ç‰ˆæœ¬ - é€‚ç”¨äºå®æ—¶æ£€æµ‹åœºæ™¯,æ·±å±‚"""
    
    def __init__(self, c1, c2, reduction=16):
        super().__init__()
        self.c1 = c1
        self.c2 = c2
        
        # ç®€åŒ–çš„åŠ¨æ€æƒé‡ç”Ÿæˆ
        self.weight_gen = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(c1, c1 // reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(c1 // reduction, c2, 1),
            nn.Sigmoid()
        )
        
        # è®¡ç®—é€šé“æ•°ï¼ˆç¡®ä¿ä¸ºæ•´æ•°ï¼‰
        branch1_channels = int(c2 * 0.75)  # è½¬æ¢ä¸ºæ•´æ•°
        branch2_channels = c2 - branch1_channels  # ç¡®ä¿æ€»å’Œç­‰äºc2
        
        # åŒåˆ†æ”¯ç‰¹å¾æå–
        self.branch1 = nn.Sequential(
            Conv(c1, branch1_channels, 1, 1),
            Conv(branch1_channels, branch1_channels, 1, 1)
        )
        
        self.branch2 = nn.Sequential(
            Conv(c1, branch2_channels, 1, 1),
            nn.Conv2d(branch2_channels, branch2_channels, 3, 1, 2, dilation=2)
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = Conv(c2, c2, 1, 1)
        
        # æ®‹å·®è¿æ¥
        self.use_residual = (c1 == c2)
        if not self.use_residual:
            self.residual_proj = Conv(c1, c2, 1, 1)
            
    def forward(self, x):
        # åŠ¨æ€æƒé‡
        weight = self.weight_gen(x)
        
        # åŒåˆ†æ”¯å¤„ç†
        feat1 = self.branch1(x)
        feat2 = self.branch2(x)
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([feat1, feat2], dim=1)
        output = self.fusion(combined) * weight
        
        # æ®‹å·®è¿æ¥
        if self.use_residual:
            output = output + x
        elif hasattr(self, 'residual_proj'):
            output = output + self.residual_proj(x)
            
        return output

class MSFAM_Ultra_Lite_Adaptive(nn.Module):
    """MSFAMè¶…è½»é‡è‡ªé€‚åº”ç‰ˆæœ¬ - åŸºäºFCMé€šé“åˆ’åˆ†ç­–ç•¥"""
    
    def __init__(self, c1, c2, depth_level='medium', reduction=32):
        super().__init__()
        self.c1 = c1
        self.c2 = c2
        self.depth_level = depth_level
        
        # æ ¹æ®æ·±åº¦çº§åˆ«ç¡®å®šé€šé“åˆ†é…ç­–ç•¥ï¼ˆå‚è€ƒFCMè®¾è®¡ï¼‰
        if depth_level == 'shallow':  # æµ…å±‚ - ç±»ä¼¼FCM_3
            self.complex_ratio = 0.75  # 75%ç”¨äºå¤æ‚å¤„ç†
            self.simple_ratio = 0.25   # 25%ç”¨äºç®€å•å¤„ç†
            reduction_factor = 16      # è¾ƒå°çš„å‹ç¼©æ¯”
        elif depth_level == 'medium':  # ä¸­å±‚ - å¹³è¡¡è®¾è®¡
            self.complex_ratio = 0.5   # 50%ç”¨äºå¤æ‚å¤„ç†
            self.simple_ratio = 0.5    # 50%ç”¨äºç®€å•å¤„ç†
            reduction_factor = 24      # ä¸­ç­‰å‹ç¼©æ¯”
        else:  # 'deep' - æ·±å±‚ï¼Œç±»ä¼¼FCM_1
            self.complex_ratio = 0.25  # 25%ç”¨äºå¤æ‚å¤„ç†
            self.simple_ratio = 0.75   # 75%ç”¨äºç®€å•å¤„ç†
            reduction_factor = 32      # è¾ƒå¤§çš„å‹ç¼©æ¯”
        
        # è®¡ç®—å®é™…é€šé“æ•°
        self.complex_channels = max(int(c1 * self.complex_ratio), 8)
        self.simple_channels = c1 - self.complex_channels
        mid_channels = c2 // 2
        
        # 1. è‡ªé€‚åº”åŠ¨æ€æƒé‡ç”Ÿæˆï¼ˆæ ¹æ®æ·±åº¦è°ƒæ•´å¤æ‚åº¦ï¼‰
        self.adaptive_weight_gen = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(c1, max(c1 // reduction_factor, 8), 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(max(c1 // reduction_factor, 8), c2, 1),
            nn.Sigmoid()
        )
        
        # 2. å¤æ‚ç‰¹å¾åˆ†æ”¯ï¼ˆå¤„ç†å¤æ‚æ¨¡å¼ï¼‰
        if self.complex_channels > 0:
            self.complex_branch = nn.Sequential(
                nn.Conv2d(self.complex_channels, mid_channels, 1),  # é™ç»´
                nn.Conv2d(mid_channels, mid_channels, 3, 1, 1, groups=mid_channels),  # æ·±åº¦å¯åˆ†ç¦»
                nn.Conv2d(mid_channels, mid_channels, 3, 1, 1, groups=mid_channels),  # é¢å¤–å¤„ç†å±‚
                nn.Conv2d(mid_channels, mid_channels, 1)  # å‡ç»´
            )
        
        # 3. ç®€å•ç‰¹å¾åˆ†æ”¯ï¼ˆå¤„ç†ç®€å•æ¨¡å¼ï¼‰
        if self.simple_channels > 0:
            self.simple_branch = nn.Sequential(
                nn.Conv2d(self.simple_channels, mid_channels, 1),  # é™ç»´
                nn.Conv2d(mid_channels, mid_channels, 3, 1, 2, dilation=2, groups=mid_channels),  # æ·±åº¦å¯åˆ†ç¦»ç©ºæ´å·ç§¯
                nn.Conv2d(mid_channels, mid_channels, 1)  # å‡ç»´
            )
        
        # 4. è‡ªé€‚åº”ç‰¹å¾èåˆ
        self.adaptive_fusion = nn.Sequential(
            nn.Conv2d(c2, c2, 1),
            nn.BatchNorm2d(c2) if depth_level != 'deep' else nn.Identity()  # æ·±å±‚ç®€åŒ–BN
        )
        
        # 5. æ®‹å·®è¿æ¥
        self.use_residual = (c1 == c2)
        if not self.use_residual:
            self.residual_proj = nn.Conv2d(c1, c2, 1)
    
    def forward(self, x):
        # åŠ¨æ€æƒé‡ç”Ÿæˆ
        dynamic_weight = self.adaptive_weight_gen(x)
        
        # è‡ªé€‚åº”é€šé“åˆ†å‰²
        if self.complex_channels > 0 and self.simple_channels > 0:
            x_complex, x_simple = torch.split(x, [self.complex_channels, self.simple_channels], dim=1)
            
            # åˆ†æ”¯å¤„ç†
            complex_feat = self.complex_branch(x_complex)
            simple_feat = self.simple_branch(x_simple)
            
            # ç‰¹å¾èåˆ
            combined_feat = torch.cat([complex_feat, simple_feat], dim=1)
        elif self.complex_channels > 0:
            # åªæœ‰å¤æ‚åˆ†æ”¯
            complex_feat = self.complex_branch(x)
            combined_feat = torch.cat([complex_feat, torch.zeros_like(complex_feat)], dim=1)
        else:
            # åªæœ‰ç®€å•åˆ†æ”¯
            simple_feat = self.simple_branch(x)
            combined_feat = torch.cat([torch.zeros_like(simple_feat), simple_feat], dim=1)
        
        # è‡ªé€‚åº”èåˆå’Œæƒé‡è°ƒåˆ¶
        output = self.adaptive_fusion(combined_feat) * dynamic_weight
        
        # æ®‹å·®è¿æ¥
        if self.use_residual:
            output = output + x
        elif hasattr(self, 'residual_proj'):
            output = output + self.residual_proj(x)
        
        return output

class PRSM(nn.Module):
    """ç»ˆæä¼˜åŒ–ç‰ˆPRSM(Parallel Receptive Sensing Module)
    
    ç‰¹ç‚¹ï¼š
    1. å¹¶è¡Œå¤šå°ºåº¦å¤„ç† + ä¸²è¡Œæ·±åº¦ç‰¹å¾æå–
    2. è½»é‡çº§åŒé‡æ³¨æ„åŠ›æœºåˆ¶
    3. è‡ªé€‚åº”ç‰¹å¾èåˆ
    4. ç‰¹å¾é‡ç”¨å’Œå‚æ•°å…±äº«
    5. å‚æ•°å¢é•¿ä»…15%ï¼Œæ€§èƒ½æå‡æ˜¾è‘—
    """
    
    def __init__(self, dim, dim_out, reduction=8):
        super().__init__()
        
        # è¾“å…¥é¢„å¤„ç†
        self.input_norm = nn.BatchNorm2d(dim)
        
        # å…±äº«ç‰¹å¾æå–å™¨ï¼ˆå‡å°‘å‚æ•°ï¼‰
        self.shared_dw = nn.Conv2d(dim, dim, 3, 1, 1, groups=dim, bias=False)
        self.shared_bn = nn.BatchNorm2d(dim)
        self.shared_act = nn.SiLU(inplace=True)
        
        # å¹¶è¡Œå¤šå°ºåº¦åˆ†æ”¯ï¼ˆå‚æ•°é‡å¾ˆå°‘ï¼‰
        self.scale_3x3 = nn.Identity()  # å¤ç”¨shared_dwçš„3x3
        self.scale_5x5 = nn.Conv2d(dim, dim, 5, 1, 2, groups=dim, bias=False)
        self.scale_7x7 = nn.Conv2d(dim, dim, 7, 1, 3, groups=dim, bias=False)
        
        # ç‚¹å·ç§¯ç‰¹å¾å˜æ¢
        self.pw_conv1 = nn.Conv2d(dim, dim, 1, bias=False)
        self.pw_bn1 = nn.BatchNorm2d(dim)
        self.pw_act1 = nn.SiLU(inplace=True)
        
        self.pw_conv2 = nn.Conv2d(dim, dim, 1, bias=False)
        self.pw_bn2 = nn.BatchNorm2d(dim)
        
        # è½»é‡çº§åŒé‡æ³¨æ„åŠ›
        self.channel_att = LightweightChannelAttention(dim, reduction)
        self.spatial_att = LightweightSpatialAttention()
        
        # è‡ªé€‚åº”ç‰¹å¾èåˆ
        self.adaptive_fusion = AdaptiveFusion(dim, 3)
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Conv2d(dim, dim, 1, bias=False)
        self.output_bn = nn.BatchNorm2d(dim)
        self.output_act = nn.SiLU(inplace=True)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        identity = x
        
        # è¾“å…¥é¢„å¤„ç†
        x = self.input_norm(x)
        
        # å…±äº«ç‰¹å¾æå–
        shared_feat = self.shared_act(self.shared_bn(self.shared_dw(x)))
        
        # å¹¶è¡Œå¤šå°ºåº¦å¤„ç†
        feat_3x3 = self.scale_3x3(shared_feat)  # å¤ç”¨shared_dwç»“æœ
        feat_5x5 = self.scale_5x5(shared_feat)
        feat_7x7 = self.scale_7x7(shared_feat)
        
        # è‡ªé€‚åº”èåˆå¤šå°ºåº¦ç‰¹å¾
        fused_feat = self.adaptive_fusion([feat_3x3, feat_5x5, feat_7x7])
        
        # ç¬¬ä¸€æ¬¡ç‚¹å·ç§¯å˜æ¢
        x = self.pw_act1(self.pw_bn1(self.pw_conv1(fused_feat)))
        
        # é€šé“æ³¨æ„åŠ›
        x = self.channel_att(x)
        
        # ç¬¬äºŒæ¬¡ç‚¹å·ç§¯å˜æ¢
        x = self.pw_bn2(self.pw_conv2(x))
        
        # ç©ºé—´æ³¨æ„åŠ›
        x = self.spatial_att(x)
        
        # è¾“å‡ºæŠ•å½±
        x = self.output_act(self.output_bn(self.output_proj(x)))
        
        # æ®‹å·®è¿æ¥
        return x + identity

class LightweightChannelAttention(nn.Module):
    """è½»é‡çº§é€šé“æ³¨æ„åŠ›ï¼Œå‚æ•°é‡æå°‘"""
    def __init__(self, channels, reduction=8):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, 1, bias=False),
            nn.SiLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return x * self.fc(self.avg_pool(x))

class LightweightSpatialAttention(nn.Module):
    """è½»é‡çº§ç©ºé—´æ³¨æ„åŠ›ï¼Œå‡ ä¹æ— å‚æ•°"""
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(2, 1, 7, padding=3, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attention = self.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1)))
        return x * attention

class AdaptiveFusion(nn.Module):
    """è‡ªé€‚åº”ç‰¹å¾èåˆ"""
    def __init__(self, channels, num_branches):
        super().__init__()
        self.num_branches = num_branches
        self.weight_gen = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, num_branches, 1),
            nn.Softmax(dim=1)
        )
    
    def forward(self, features):
        # features: list of [B, C, H, W]
        stacked = torch.stack(features, dim=2)  # [B, C, num_branches, H, W]
        weights = self.weight_gen(features[0])  # [B, num_branches, 1, 1]
        
        # è°ƒæ•´æƒé‡ç»´åº¦ä»¥åŒ¹é…stackedç‰¹å¾
        weights = weights.unsqueeze(1)  # [B, 1, num_branches, 1, 1]
        
        # åŠ æƒèåˆ
        fused = torch.sum(stacked * weights, dim=2)  # [B, C, H, W]
        return fused